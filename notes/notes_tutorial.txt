
\section{NOTES}

ML-lib: https://spark.apache.org/mllib/

Setting up Google cloud for using Spark and Hadoop:

(https://cloud.google.com/dataproc/docs/tutorials/bigquery-sparkml)

- Creating Google Dataproc Cluster (https://towardsdatascience.com/step-by-step-tutorial-pyspark-sentiment-analysis-on-google-dataproc-fef9bef46468)
Cloud Dataproc is a Google cloud service for running Apache Spark and Apache Hadoop clusters. I have to say it is ridiculously simple and easy-to-use and it only takes a couple of minutes to spin up a cluster with Google Dataproc. Also, Google Dataproc offers autoscaling if you need, and you can adjust the cluster at any time, even when jobs are running on the cluster.

Go to Dataproc from the left side menu (you have to scroll down a bit. It’s under Big Data section) and click on “Clusters”. Click “Create clusters”. Give it a name (for convenience, I gave the project ID as its name), choose Region and Zone. To decrease the latency, it is a good idea to set the region to be the same as your bucket region. Here you need to change the default settings for worker nodes a little, as the free trial only gives you permission to run up to 8 cores. The default setting for a cluster is one master and two workers all with 4 CPUs each, which will exceed the 8 cores quota. So change the setting for your worker nodes to 2 CPUs, then click create at the bottom. After a couple of minutes of provisioning, you will see the cluster created with one master node (4 CPUs, 15GB memory, 500GB standard persistent disk) and two worker nodes (2 CPUs, 15GB memory, 500GB standard persistent disk each).


ML-Lib Guide:
https://spark.apache.org/docs/2.0.0/ml-guide.html

http://www.blogforbrains.com/blog/2014/9/6/loading-matlab-mat-data-in-python
https://stackoverflow.com/questions/874461/read-mat-files-in-python
http://blog.madhukaraphatak.com/matfile-to-rdd/
https://stackoverflow.com/questions/40645498/create-sparse-rdd-from-scipy-sparse-matrix

Create Spark matrix: (https://spark.apache.org/docs/1.5.1/api/java/org/apache/spark/mllib/linalg/SparseMatrix.html)

SparseMatrix(int numRows, int numCols, int[] colPtrs, int[] rowIndices, double[] values) 


Possible operations:

(https://stackoverflow.com/questions/32704333/basic-linear-algebra-on-spark-matrices)

As for now (Spark 1.6.0) pyspark.mllib.linalg.distributed API is limited to basic operations like counting rows/columns and transformations between types. 
Scala API supports a broader set of methods including multiplication (RowMatrix.multiply, Indexed.RowMatrix.multiply), transposition, SVD (IndexedRowMatrix.computeSVD), QR decomposition (RowMatrix.tallSkinnyQR), Grammian Matrix computation (computeGramianMatrix), PCA (RowMatrix.computePrincipalComponents) which can be used to implement more complex linear algebra functions.